<!DOCTYPE html>
<html>

<head>
    <meta charset="utf-8">
    <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
    <!-- Replace the content tag with appropriate information -->
    <meta name="description" content="We introduce AnyV2V, A Unified Framework for Video-to-Video Generation Tasks.">
    <meta property="og:title" content="AnyV2V" />
    <meta property="og:description" content="A Unified Framework for Video-to-Video Generation Tasks." />
    <meta property="og:url" content="https://tiger-ai-lab.github.io/AnyV2V/" />
    <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
    <meta property="og:image" content="static/images/banner.png" />
    <meta property="og:image:width" content="1200" />
    <meta property="og:image:height" content="630" />


    <meta name="twitter:title" content="AnyV2V">
    <meta name="twitter:description" content="A Unified Framework for Video-to-Video Generation Tasks.">
    <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
    <meta name="twitter:image" content="static/images/banner.png">
    <meta name="twitter:card" content="summary_large_image">
    <!-- Keywords for your paper to be indexed by-->
    <meta name="keywords" content="AnyV2V">
    <meta name="viewport" content="width=device-width, initial-scale=1">


    <title>AnyV2V</title>
    <link rel="icon" type="image/x-icon" href="static/images/icon.png">
    <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

    <link rel="stylesheet" href="static/css/bulma.min.css">
    <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    <link rel="stylesheet" href="static/css/index.css">

    <script src="static/js/jquery.min.js"></script>
    <script src="static/js/main.js"></script>
    <script defer src="static/js/fontawesome.all.min.js"></script>
    <script src="static/js/bulma-carousel.min.js"></script>
    <script src="static/js/bulma-slider.min.js"></script>
    <script src="static/js/index.js"></script>

    <link rel="stylesheet" type="text/css" href="static/css/jquery.dataTables.css">
    <script type="text/javascript" charset="utf8" src="static/js/jquery-3.5.1.js"></script>
    <script type="text/javascript" charset="utf8" src="static/js/jquery.dataTables.js"></script>
</head>

<body>


    <section class="hero">
        <div class="hero-body">
            <div class="container is-max-desktop">
                <div class="columns is-centered">
                    <div class="column has-text-centered">
                        <h1 class="title is-1 publication-title" style="display: flex; align-items: center;">
                            <img src="static/images/icon.png" alt="AnyV2V Icon" style="flex: 0 0 auto; height: 2em; margin-right: 10px;">
                            <span style="flex: 1 1 auto;">AnyV2V: A Plug-and-Play Framework For Any Video-to-Video Editing Tasks</span>
                          </h1>                          
                          
                        <div class="is-size-5 publication-authors">
                            <!-- Paper authors -->
                            <span class="author-block">
                <sup>♠️†</sup><a href="https://kuwingfung.github.io/" target="_blank">Max Ku</a>*,</span>
                            <span class="author-block">
                  <sup>♠️†</sup><a href="https://congwei1230.github.io/" target="_blank">Cong Wei</a>*,
                            <span class="author-block">
                    <sup>♠️†</sup><a href="https://cs.uwaterloo.ca/~w2ren/" target="_blank">Weiming Ren</a>*,
                  </span>
                            <span class="author-block">
                    <sup>♥</sup><a href="" target="_blank">Huan Yang</a>,
                  </span>
                            <span class="author-block">
                    <sup>♠️†</sup><a href="https://wenhuchen.github.io/" target="_blank">Wenhu Chen</a>
                  </span>
                        </div>

                        <div class="is-size-5 publication-authors">
                            <span class="author-block">
                    <sup>♠️</sup>University of Waterloo,
                    <sup>†</sup>Vector Institute,
                    <sup>♥</sup>Harmony.AI
                            </span>
                            <br>
                            <span class="author-block"><small>*Equal contribution</small></span>
                            <br>
                            <span class="author-block"><small>{m3ku, w2ren, c58wei, wenhuchen}@uwaterloo.ca</small></span>
                        </div>

                        <div class="column has-text-centered">
                            <div class="publication-links">

                                <!-- Github link -->
                                <span class="link-block">
                      <a href="https://github.com/TIGER-AI-Lab/AnyV2V" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fab fa-github"></i>
                      </span>
                                <span>Code (Coming Soon)</span>
                                </a>
                                </span>

                                <!-- ArXiv abstract Link -->
                                <span class="link-block">
                    <a href="" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="ai ai-arxiv"></i>
                      </span>
                                <span>arXiv (Coming Soon)</span>
                                </a>
                                </span>
                                
                                
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </section>

    <section class="hero">
        <div class="hero-body">
            <div class="container is-max-desktop">
                <div class="columns is-centered">
                    <div class="column is-full">
                        <div class="item">
                            <!-- Your image here -->
                            <img src="static/images/banner.png" width="100%" alt="AnyV2V" />
                            <h2 class="subtitle">
                                AnyV2V disentangles the video editing process into two stages: (1) first-frame image editing and (2) image-to-video reconstruction. The first phase benefits from the extensive range of existing image editing models, enabling (1) detailed and precise modification and (2) flexibility for any editing tasks.
                            </h2>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </section>

    <section class="hero is-light is-small">
        <div class="hero-body">
          <div class="container">
            
            <video style="display: block; margin: auto; max-height: 50vh;" poster="" id="" autoplay controls muted loop playsinline height="100%">
                <source src="./static/videos/old_man/src.mp4"
                        type="video/mp4">
            </video>
            Edited with AnyV2V:
            <div id="results-carousel" class="carousel results-carousel">
              <div class="item">
                <video poster="" id="" autoplay controls muted loop playsinline height="100%">
                  <source src="./static/videos/old_man/row_sub1.mp4"
                          type="video/mp4">
                </video>
              </div>
              <div class="item">
                <video poster="" id="" autoplay controls muted loop playsinline height="100%">
                  <source src="./static/videos/old_man/row_sub2.mp4"
                          type="video/mp4">
                </video>
              </div>
            </div>
          </div>
        </div>
    </section>

    <!-- Paper abstract -->
    <section class="hero">
        <div class="hero-body">
            <div class="container is-max-desktop">
                <div class="columns is-centered">
                    <div class="column has-text-centered">
                        <h2 class="title is-3">Abstract</h2>
                        <div class="content has-text-justified">
                            <p>
                                Video-to-video editing involves editing a source video along with additional control (such as text prompts, subjects, or styles) to generate a new video that aligns with the source video and the provided control. Traditional methods have been constrained to certain editing types, limiting their ability to meet the wide range of user demands. In this paper, we introduce AnyV2V, a novel training-free framework designed to simplify video editing into two primary steps: (1) employing an off-the-shelf image editing model (e.g. InstructPix2Pix, InstantID, etc) to modify the first frame, (2) utilizing an existing image-to-video generation model (e.g. I2VGen-XL) for DDIM inversion and feature injection.
In the first stage, AnyV2V can plug in any existing image editing tools to support an extensive array of video editing tasks. Beyond the traditional prompt-based editing methods, AnyV2V also can support novel video editing tasks, including reference-based style transfer, subject-driven editing, and identity manipulation, which were unattainable by previous methods. In the second stage, AnyV2V can plug in any existing image-to-video models to perform DDIM inversion and intermediate feature injection to maintain the appearance and motion consistency with the source video. On the prompt-based editing, we show that AnyV2V can outperform the previous best approach by 35\% on prompt alignment, and 25\% on human preference. On the three novel tasks, we show that AnyV2V also achieves a high success rate. We believe AnyV2V will continue to thrive due to its ability to seamlessly integrate the fast-evolving image editing methods. Such compatibility can help AnyV2V to increase its versatility to cater to diverse user demands.
                            </p>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </section>
    <!-- End paper abstract -->


      <section class="section">
        <div class="container is-max-desktop">
      
          <div class="columns is-centered">
      
            <div class="column">
                <div class="content">
                  <h2 class="title is-3">Prompt based Editing</h2>
                  <p>
                    AnyV2V is robust in a wide range of localized editing tasks while maintaining the background. The generated result aligns the most with the text prompt and also maintains high motion consistency.
                  </p>
                  <!-- Text above Video 1 -->
                  <div style="text-align: center; margin-bottom: 2px;">"..., snowing"</div>
                  <video id="" autoplay controls muted loop playsinline height="100%">
                    <source src="./static/videos/prompt_based/row1.mp4" type="video/mp4">
                  </video>
              
                  <!-- Text above Video 2 -->
                  <div style="text-align: center; margin-top: 2px; margin-bottom: 2px;">"turn the man into darth vader"</div>
                  <video id="" autoplay controls muted loop playsinline height="100%">
                      <source src="./static/videos/prompt_based/row2.mp4" type="video/mp4">
                  </video>
              
                  <!-- Text above Video 3 -->
                  <div style="text-align: center; margin-top: 2px; margin-bottom: 2px;">"turn the couple into robots"</div>
                  <video id="" autoplay controls muted loop playsinline height="100%">
                      <source src="./static/videos/prompt_based/row3.mp4" type="video/mp4">
                  </video>

                  <div style="text-align: center; margin-top: 2px; margin-bottom: 2px;">"turn the sand into snow"</div>
                  <video id="" autoplay controls muted loop playsinline height="100%">
                      <source src="./static/videos/prompt_based/row4.mp4" type="video/mp4">
                  </video>
                </div>
              </div>
              

            <div class="column">
              <h2 class="title is-3">Subject Driven Editing</h2>
              <div class="columns is-centered">
                <div class="column content">
                  <p>
                    AnyV2V works well in subject-driven editing tasks, where the user can specify the subject to be edited. The model can generate high-quality results with the subject being edited while maintaining the background.
                  </p>
                  Subject refernce (only 1 image):
                  <img src="./static//videos/subject_driven/v-car.png" width="40%" alt="style_1" />
                  <video id="" autoplay controls muted loop playsinline height="100%">
                      <source src="./static/videos/subject_driven/row1.mp4"
                              type="video/mp4">
                  </video>
                  Subject refernce (only 1 image):
                  <img src="./static//videos/subject_driven/v-dog.jpg" width="40%" alt="style_1" />
                  <video id="" autoplay controls muted loop playsinline height="100%">
                      <source src="./static/videos/subject_driven/row2.mp4"
                              type="video/mp4">
                  </video>
                  Subject refernce (only 1 image):
                  <img src="./static//videos/subject_driven/v-cat.jpg" width="40%" alt="style_1" />
                  <video id="" autoplay controls muted loop playsinline height="100%">
                      <source src="./static/videos/subject_driven/row3.mp4"
                              type="video/mp4">
                  </video>
                </div>
      
              </div>
            </div>

          </div>

          <div class="columns is-centered">
                
            <div class="column">
                <div class="content">
                <h2 class="title is-3">Style Transfer</h2>
                <p>
                    AnyV2V can perform reference-based style transfer tasks on styles that is never learned in a text encoder. The existing models lack such capability.
                </p>
                <img src="./static//videos/style_transfer/Vassily_Kandinsky.jpg" width="100%" alt="style_1" />
                <video id="" autoplay controls muted loop playsinline height="100%">
                    <source src="./static/videos/style_transfer/row1.mp4"
                            type="video/mp4">
                </video>
                </div>
            </div>

            <div class="column">
                <h2 class="title is-3">Identity Manipulation</h2>
                <div class="columns is-centered">
                <div class="column content">
                    <p>
                        AnyV2V can perform identity manipulation tasks based on InstantID, while such applications are not possible for existing methods.
                    </p>
                    <img src="./static//videos/id/wenhu.jpg" width="50%" alt="" /><img src="./static//videos/id/instantid_wenhu.png" width="50%" alt="" />
                    <video id="" autoplay controls muted loop playsinline height="100%">
                        <source src="./static/videos/id/row1.mp4"
                                type="video/mp4">
                    </video>
                </div>
        
                </div>
            </div>

        </div>


        <div class="column">
            <div class="column has-text-centered">
                <h2 class="title is-3">Comparison</h2>
                <p>
                    AnyV2V can preserve the video fidelity while performing the correct amount of edit.
                </p>
                <table class="table is-fullwidth is-striped is-hoverable">
                    <tbody>
                        <tr>
                            <td style="text-align: center;"><div style="margin-bottom: 2px;">Source</div></td>
                            <td style="text-align: center;"><div style="margin-bottom: 2px;">AnyV2V(Ours)</div></td>
                            <td style="text-align: center;"><div style="margin-bottom: 2px;">TokenFlow</div></td>
                            <td style="text-align: center;"><div style="margin-bottom: 2px;">FLATTEN</div></td>
                        </tr>
                        <tr>
                            <td style="text-align: center;">
                                <video autoplay controls muted loop playsinline height="100%">
                                    <source src="./static/videos/comparison/snow/src.mp4" type="video/mp4">
                                </video>
                            </td>
                            <td style="text-align: center;">
                                <video autoplay controls muted loop playsinline height="100%">
                                    <source src="./static/videos/comparison/snow/v2v_i2vgenxl.mp4" type="video/mp4">
                                </video>
                            </td>
                            <td style="text-align: center;">
                                <video autoplay controls muted loop playsinline height="100%">
                                    <source src="./static/videos/comparison/snow/tokenflow.mp4" type="video/mp4">
                                </video>
                            </td>
                            <td style="text-align: center;">
                                <video autoplay controls muted loop playsinline height="100%">
                                    <source src="./static/videos/comparison/snow/flatten.mp4" type="video/mp4">
                                </video>
                            </td>
                        </tr>
                        <tr>
                            <td style="text-align: center;">
                                
                                <video autoplay controls muted loop playsinline height="100%">
                                    <source src="./static/videos/comparison/man_hat/src.mp4" type="video/mp4">
                                </video>
                            </td>
                            <td style="text-align: center;">
                                <video autoplay controls muted loop playsinline height="100%">
                                    <source src="./static/videos/comparison/man_hat/v2v_i2vgenxl.mp4" type="video/mp4">
                                </video>
                            </td>
                            <td style="text-align: center;">
                                <video autoplay controls muted loop playsinline height="100%">
                                    <source src="./static/videos/comparison/man_hat/tokenflow.mp4" type="video/mp4">
                                </video>
                            </td>
                            <td style="text-align: center;">
                                <video autoplay controls muted loop playsinline height="100%">
                                    <source src="./static/videos/comparison/man_hat/flatten.mp4" type="video/mp4">
                                </video>
                            </td>
                        </tr>
                        <tr>
                            <td style="text-align: center;">
                                
                                <video autoplay controls muted loop playsinline height="100%">
                                    <source src="./static/videos/comparison/robot_couple/src.mp4" type="video/mp4">
                                </video>
                            </td>
                            <td style="text-align: center;">
                                <video autoplay controls muted loop playsinline height="100%">
                                    <source src="./static/videos/comparison/robot_couple/v2v_i2vgenxl.mp4" type="video/mp4">
                                </video>
                            </td>
                            <td style="text-align: center;">
                                <video autoplay controls muted loop playsinline height="100%">
                                    <source src="./static/videos/comparison/robot_couple/tokenflow.mp4" type="video/mp4">
                                </video>
                            </td>
                            <td style="text-align: center;">
                                <video autoplay controls muted loop playsinline height="100%">
                                    <source src="./static/videos/comparison/robot_couple/flatten.mp4" type="video/mp4">
                                </video>
                            </td>
                        </tr>

                    </tbody>
                </table>

                <table class="table is-fullwidth is-striped is-hoverable">
                    <tbody>
                        <tr>
                            <td style="text-align: center;"><div style="margin-bottom: 2px;">Source</div></td>
                            <td style="text-align: center;"><div style="margin-bottom: 2px;">Reference</div></td>
                            <td style="text-align: center;"><div style="margin-bottom: 2px;">AnyV2V(Ours)</div></td>
                            <td style="text-align: center;"><div style="margin-bottom: 2px;">VideoSwap</div></td>
                        </tr>
                        <tr>
                            <td style="text-align: center;">
                                <video autoplay controls muted loop playsinline height="100%">
                                    <source src="./static/videos/comparison/V_dog/src.mp4" type="video/mp4">
                                </video>
                            </td>
                            <td style="text-align: center;">
                                <img src="static/videos/subject_driven/v-dog.jpg" width="512" alt="AnyV2V" />
                            </td>
                            <td style="text-align: center;">
                                <video autoplay controls muted loop playsinline height="100%">
                                    <source src="./static/videos/comparison/V_dog/v2v_i2vgenxl.mp4" type="video/mp4">
                                </video>
                            </td>
                            <td style="text-align: center;">
                                <video autoplay controls muted loop playsinline height="100%">
                                    <source src="./static/videos/comparison/V_dog/videoswap.mp4" type="video/mp4">
                                </video>
                            </td>
                        </tr>
                        <tr>
                            <td style="text-align: center;">
                                <video autoplay controls muted loop playsinline height="100%">
                                    <source src="./static/videos/comparison/V_dog2/src.mp4" type="video/mp4">
                                </video>
                            </td>
                            <td style="text-align: center;">
                                <img src="static/videos/comparison/V_dog2/v-dog-01.jpg" width="512" alt="AnyV2V" />
                            </td>
                            <td style="text-align: center;">
                                <video autoplay controls muted loop playsinline height="100%">
                                    <source src="./static/videos/comparison/V_dog2/v2v_i2vgenxl.mp4" type="video/mp4">
                                </video>
                            </td>
                            <td style="text-align: center;">
                                <video autoplay controls muted loop playsinline height="100%">
                                    <source src="./static/videos/comparison/V_dog2/videoswap.mp4" type="video/mp4">
                                </video>
                            </td>
                        </tr>

                    </tbody>
                </table>
            </div>
        </div>

    <section class="hero is-small">
        <div class="hero-body">
            <div class="container  is-max-desktop">
                <div class="columns is-centered has-text-centered">
                    <div class="column is-four-fifths">
                        <h2 class="title is-3">Technical Detail</h2>
                        <div class="item">
                            <img src="static/images/method.png" width="100%" alt="method" />
                            <h2 class="subtitle">
                                AnyV2V framework. Our framework takes a source video as input.
                                In the first stage, we apply a block-box image editing method on the first frame according to the editing task.
                                In the second stage, the source video is inverted to initial noise, which is then denoised using DDIM sampling.
                                During the sampling process, we extract spatial features, spatial attention and temporal attention from the image-to-video' decoder layers. To generate our edited video, we perform a DDIM sampling by fixing the latent and use the edited first frame as the conditional signal. During the sampling, we inject the features and attention into corresponding layers of the model.
                            </h2>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </section>


    <!-- BibTex citation -->
    <section class="section" id="BibTeX">
        <div class="container is-max-desktop content">
            <h2 class="title">Citation</h2>
            Please kindly cite our paper if you use our code, data, models or results:
            <br><br>
    <pre><code>@article{
        ku2024anyv2v,
        title={AnyV2V: A Plug-and-Play Framework For Any Video-to-Video Editing Tasks},
        author={Max Ku and Cong Wei and Weiming Ren and Huan Yang and Wenhu Chen},
        journal={arXiv preprint arXiv:_______},
        year={2024}
        }
    </code></pre>
        </div>
    </section>


    <footer class="footer">
        <div class="container">
            <div class="columns is-centered">
                <div class="column is-8">
                    <div class="content">

                        <p>
                            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a>                            project page. You are free to borrow the of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/"
                                target="_blank">Creative
              Commons Attribution-ShareAlike 4.0 International License</a>.
                        </p>

                    </div>
                </div>
            </div>
        </div>
    </footer>

</body>
<style>
    .buttonGroup {
        text-align: center;
    }
    
    .buttonGroup>button {
        padding: 15px;
        color: white;
        background-color: #363636;
        border-radius: 5px;
    }
    
    .buttonGroup>button:hover {
        box-shadow: 5px;
    }
</style>

</html>
