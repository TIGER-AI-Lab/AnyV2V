<!DOCTYPE html>
<html>

<head>
    <meta charset="utf-8">
    <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
    <!-- Replace the content tag with appropriate information -->
    <meta name="description" content="We introduce UniV2V, A Unified Framework for Video-to-Video Generation Tasks.">
    <meta property="og:title" content="UniV2V" />
    <meta property="og:description" content="A Unified Framework for Video-to-Video Generation Tasks." />
    <meta property="og:url" content="https://tiger-ai-lab.github.io/UniV2V/" />
    <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
    <meta property="og:image" content="static/images/banner.png" />
    <meta property="og:image:width" content="1200" />
    <meta property="og:image:height" content="630" />


    <meta name="twitter:title" content="UniV2V">
    <meta name="twitter:description" content="A Unified Framework for Video-to-Video Generation Tasks.">
    <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
    <meta name="twitter:image" content="static/images/banner.png">
    <meta name="twitter:card" content="summary_large_image">
    <!-- Keywords for your paper to be indexed by-->
    <meta name="keywords" content="univ2v">
    <meta name="viewport" content="width=device-width, initial-scale=1">


    <title>UniV2V</title>
    <link rel="icon" type="image/x-icon" href="static/images/favicon_io/favicon.ico">
    <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

    <link rel="stylesheet" href="static/css/bulma.min.css">
    <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    <link rel="stylesheet" href="static/css/index.css">

    <script src="static/js/jquery.min.js"></script>
    <script src="static/js/main.js"></script>
    <script defer src="static/js/fontawesome.all.min.js"></script>
    <script src="static/js/bulma-carousel.min.js"></script>
    <script src="static/js/bulma-slider.min.js"></script>
    <script src="static/js/index.js"></script>

    <link rel="stylesheet" type="text/css" href="static/css/jquery.dataTables.css">
    <script type="text/javascript" charset="utf8" src="static/js/jquery-3.5.1.js"></script>
    <script type="text/javascript" charset="utf8" src="static/js/jquery.dataTables.js"></script>
</head>

<body>


    <section class="hero">
        <div class="hero-body">
            <div class="container is-max-desktop">
                <div class="columns is-centered">
                    <div class="column has-text-centered">
                        <h1 class="title is-1 publication-title"> UniV2V: A Unified Framework for Video-to-Video Generation Tasks</h1>
                        <div class="is-size-5 publication-authors">
                            <!-- Paper authors -->
                            <span class="author-block">
                <sup>♠️†</sup><a href="https://kuwingfung.github.io/" target="_blank">Max Ku</a>*',</span>
                            <span class="author-block">
                  <sup>♠️†</sup><a href="https://congwei1230.github.io/" target="_blank">Cong Wei</a>*,
                            <span class="author-block">
                    <sup>♠️†</sup><a href="https://cs.uwaterloo.ca/~w2ren/" target="_blank">Weiming Ren</a>*,
                  </span>
                            <span class="author-block">
                    <sup>♥</sup><a href="" target="_blank">Huan Yang</a>,
                  </span>
                            <span class="author-block">
                    <sup>♠️†</sup><a href="https://wenhuchen.github.io/" target="_blank">Wenhu Chen</a>
                  </span>
                        </div>

                        <div class="is-size-5 publication-authors">
                            <span class="author-block">
                    <sup>♠️</sup>University of Waterloo,
                    <sup>†</sup>Vector Institute,
                    <sup>♥</sup>Microsoft Research
                            </span>
                            <br>
                            <span class="author-block"><small>*Equal contribution, 'Leading author</small></span>
                            <br>
                            <span class="author-block"><small>{m3ku, w2ren, c58wei, wenhuchen}@uwaterloo.ca</small></span>
                        </div>

                        <div class="column has-text-centered">
                            <div class="publication-links">

                                <!-- Github link -->
                                <span class="link-block">
                      <a href="https://github.com/TIGER-AI-Lab/UniV2V" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fab fa-github"></i>
                      </span>
                                <span>Code (Coming Soon)</span>
                                </a>
                                </span>

                                <!-- ArXiv abstract Link -->
                                <span class="link-block">
                    <a href="" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="ai ai-arxiv"></i>
                      </span>
                                <span>arXiv (Coming Soon)</span>
                                </a>
                                </span>
                                
                                
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </section>

    <section class="hero">
        <div class="hero-body">
            <div class="container is-max-desktop">
                <div class="columns is-centered">
                    <div class="column is-full">
                        <div class="item">
                            <!-- Your image here -->
                            <img src="static/images/banner.png" width="100%" alt="UniV2V" />
                            <h2 class="subtitle">
                                UniV2V disentangles the video editing process into two stages: (1) first-frame image editing and (2) image-to-video reconstruction. The first phase benefits from the extensive range of existing image editing models, enabling (1) detailed and precise modification and (2) flexibility for any editing tasks.
                            </h2>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </section>

    <section class="hero is-light is-small">
        <div class="container">
            <video poster="" id="" autoplay controls muted loop playsinline height="100%">
              <source src="./static/videos/old_man/row1.mp4"
                      type="video/mp4">
            </video>
        </div>
    </section>

    
    <!--
    <section class="hero is-light is-small">
        <div class="hero-body">
          <div class="container">
            <div id="results-carousel" class="carousel results-carousel">
              <div class="item">
                <video poster="" id="" autoplay controls muted loop playsinline height="100%">
                  <source src="./static/videos/old_man/row1.mp4"
                          type="video/mp4">
                </video>
              </div>
              <div class="item">
                <video poster="" id="" autoplay controls muted loop playsinline height="100%">
                  <source src="./static/videos/old_man/row1.mp4"
                          type="video/mp4">
                </video>
              </div>
            </div>
          </div>
        </div>
      </section>
    -->

    <!-- Paper abstract -->
    <section class="hero">
        <div class="hero-body">
            <div class="container is-max-desktop">
                <div class="columns is-centered">
                    <div class="column has-text-centered">
                        <h2 class="title is-3">Abstract</h2>
                        <div class="content has-text-justified">
                            <p>
                                Video editing is the task of conditioning on a source video plus additional guidance (text prompt, subject, style, etc) to synthesize a new video consistent with both the source video and the provided guidance. The existing approaches are limited to performing specific types of editing, which restricts their potential to cater to the diverse needs of the users. In this paper, we propose UniV2V, a new framework to decompose video editing into two stages: (1) one-frame image editing and (2) image-to-video reconstruction with source video DDIM trajectory. The first stage can benefit from the myriad of existing image editing models like InstructPix2Pix, InstantID, WISE. Our method is highly compatible, as it works with any Image-to-Video (I2V) model and covers several editing tasks, including prompt-based editing, style transfer, and identity manipulation. We conduct a comprehensive human evaluation and show that UniV2V is able to outperform the other baselines by a significant margin across the three different tasks. Due to its simplicity and compatibility, we believe our method can be easily deployed in real-world applications to fit the diverse needs of the users.
                            </p>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </section>
    <!-- End paper abstract -->


      <section class="section">
        <div class="container is-max-desktop">
      
          <div class="columns is-centered">
      
            <div class="column">
              <div class="content">
                <h2 class="title is-3">Prompt based Editing</h2>
                <p>
                    UniV2V is robust in a wide range of localized editing tasks while maintaining the background. The generated result aligns the most with the text prompt and also maintains high motion consistency.
                </p>
                <video id="" autoplay controls muted loop playsinline height="100%">
                  <source src="./static/videos/prompt_based/row1.mp4"
                          type="video/mp4">
                </video>
                <video id="" autoplay controls muted loop playsinline height="100%">
                    <source src="./static/videos/prompt_based/row2.mp4"
                            type="video/mp4">
                </video>
                <video id="" autoplay controls muted loop playsinline height="100%">
                    <source src="./static/videos/prompt_based/row3.mp4"
                            type="video/mp4">
                </video>
              </div>
            </div>

            <div class="column">
              <h2 class="title is-3">Subject Driven Editing</h2>
              <div class="columns is-centered">
                <div class="column content">
                  <p>
                    UniV2V works well in subject-driven editing tasks, where the user can specify the subject to be edited. The model can generate high-quality results with the subject being edited while maintaining the background.
                  </p>
                  Token refernce (only 1 image):
                  <img src="./static//videos/subject_driven/v-car.png" width="33%" alt="style_1" />
                  <video id="" autoplay controls muted loop playsinline height="100%">
                      <source src="./static/videos/subject_driven/row1.mp4"
                              type="video/mp4">
                  </video>
                  Token refernce (only 1 image):
                  <img src="./static//videos/subject_driven/v-dog.jpg" width="33%" alt="style_1" />
                  <video id="" autoplay controls muted loop playsinline height="100%">
                      <source src="./static/videos/subject_driven/row2.mp4"
                              type="video/mp4">
                  </video>
                </div>
      
              </div>
            </div>

          </div>

          <div class="columns is-centered">
                
            <div class="column">
                <div class="content">
                <h2 class="title is-3">Style Transfer</h2>
                <p>
                    UniV2V can perform style transfer tasks on styles that is never learned in a text encoder. The existing models lack such capability.
                </p>
                <img src="./static//videos/style_transfer/Vassily_Kandinsky.jpg" width="100%" alt="style_1" />
                <video id="" autoplay controls muted loop playsinline height="100%">
                    <source src="./static/videos/style_transfer/row1.mp4"
                            type="video/mp4">
                </video>
                </div>
            </div>

            <div class="column">
                <h2 class="title is-3">Identity Manipulation</h2>
                <div class="columns is-centered">
                <div class="column content">
                    <p>
                        UniV2V is also performing well in identity manipulation tasks while the existing models lack such capability.
                    </p>
                    <img src="./static//videos/id/wenhu.jpg" width="50%" alt="" /><img src="./static//videos/id/instantid_wenhu.png" width="50%" alt="" />
                    <video id="" autoplay controls muted loop playsinline height="100%">
                        <source src="./static/videos/id/row1.mp4"
                                type="video/mp4">
                    </video>
                </div>
        
                </div>
            </div>

        </div>

    <!--
    <section class="hero is-small">
        <div class="hero-body">
            <div class="container  is-max-desktop">
                <div class="columns is-centered has-text-centered">
                    <div class="column is-four-fifths">
                        <h2 class="title is-3">Technical Detail</h2>
                        <div class="item">
                            <img src="static/images/method.png" width="100%" alt="method" />
                            <h2 class="subtitle">
                                UniV2V framework. Our framework takes a source video as input.
                                In the first stage, we apply a block-box image editing method on the first frame according to the editing task.
                                In the second stage, the source video is inverted to initial noise, which is then denoised using DDIM sampling.
                                During the sampling process, we extract spatial features, spatial attention and temporal attention from the image-to-video' decoder layers. To generate our edited video, we perform a DDIM sampling by fixing the latent and use the edited first frame as the conditional signal. During the sampling, we inject the features and attention into corresponding layers of the model.
                            </h2>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </section>
    -->

    <!-- BibTex citation -->
    <section class="section" id="BibTeX">
        <div class="container is-max-desktop content">
            <h2 class="title">Citation</h2>
            Please kindly cite our paper if you use our code, data, models or results:
            <br><br>
            <pre><code>TBA</code></pre>
        </div>
    </section>


    <footer class="footer">
        <div class="container">
            <div class="columns is-centered">
                <div class="column is-8">
                    <div class="content">

                        <p>
                            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a>                            project page. You are free to borrow the of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/"
                                target="_blank">Creative
              Commons Attribution-ShareAlike 4.0 International License</a>.
                        </p>

                    </div>
                </div>
            </div>
        </div>
    </footer>

</body>
<style>
    .buttonGroup {
        text-align: center;
    }
    
    .buttonGroup>button {
        padding: 15px;
        color: white;
        background-color: #363636;
        border-radius: 5px;
    }
    
    .buttonGroup>button:hover {
        box-shadow: 5px;
    }
</style>

</html>
